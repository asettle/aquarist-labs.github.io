---
layout: post
title:  Aquarium Meeting Minutes 2021-05-26
date:   2021-05-26 15:00:00 +0100
categories: meetings
author:
- Aquarist Labs
---

* General
   * Tomorrow's meeting is set to primarily be around testing
       * Josh can't attend - reschedule?
       * Kyr is to be away next week - perhaps we can have something a bit earlier? Alex to coordinate with Josh
   * Backlog assigning - prioritising tasks out of bounds of the milestones
       * Joao: Does anyone care if we have more tags? (see Slack channel chat)
       * Tags are low/medium/high/urgent for issues in general (outside of the nice to have, recommended, required - for specific milestones)
       * Agreed upon to try

* Backend
   * storing cluster statistics
       * problem: prometheus does not replicate, Ceph would be running a single instance; in case of server loss, we're losing that instance. It can be reallocated, but in case of quorum failure we're losing the statistics as well.
       * question: is it worth caring about this? It would be nice to have a vision of the state of the cluster over time until the failure, but it might not be worth running yet another cluster on top of Ceph (e.g., prometheus + cortexmetrics.io) just for statistics gathering and safe-keeping. Discuss.
       * Pacific's discussion results:
           * probably good to go with what Ceph already does, reduces pain in the short term;
           * it's annoying that the system being monitored is managing the monitoring itself, but being an appliance we don't have much choice;
           * given how Ceph deploys prometheus, and prometheus nature, if we lose the host where prometheus is running we lose the statistics until that host comes back AND prometheus is restarted on the same host
               * why: prometheus keeps a bind mount from the host to store the data, but there's no replication between hosts, so if the host dies the statistics go down with it.
               * workaround: deploy multiple prometheus instances, and hope we don't lose all prometheus hosts.
           * kick this down the road a bit because we spent 30 minutes going around in circles;
       * additional input from Patrick in [https://github.com/aquarist-labs/aquarium/issues/89#issuecomment-779110879](https://github.com/aquarist-labs/aquarium/issues/89#issuecomment-779110879)
   * seminal work towards disk selection during initial deployment
       * We'll need to add these steps to the wizard to allow this to happen for a node joining the cluster as well.
       * What problems might we find when setting up a system disk?
           * Any insights on how gravel should handle it?
           * What about bind mounting system partition directories to several places in the root fs?
           * We may have a tricky thing going with starting Aquarium on start and bind mounting stuff **after** boot?
       * Should we have the user selecting what other disks they want for storage?
       * Should we gobble all remaining disks (--all-available-devices), or should we add devices individually, per host?
           * Joao: I don't think individually selecting them makes our life particularly easier now or in the long run. Thoughts?
       * Pacific's discussion results:
           * User should specify system disk;
           * It's easier to gobble every other existing disk for storage, because we are opinionated and have decided to do it that way;
           * It's probably better to have the user specify which disks they want to add though, because otherwise Ceph will gobble **any** available disk that it finds, including USB sticks that are >5GB.
           * Create a system service that checks for Aquarium lvm-tagged volumes on boot, prior to running most services, and bind mount to specific places; keep it simple for now: one single partition, bind mount directories.
   * Let's discuss what additional items we want to push for M4, and divide them amongst the team.
       * [https://github.com/aquarist-labs/aquarium/issues?q=is%3Aopen+is%3Aissue+milestone%3A%22Milestone+4%22+label%3Agravel](https://github.com/aquarist-labs/aquarium/issues?q=is%3Aopen+is%3Aissue+milestone%3A%22Milestone+4%22+label%3Agravel)
       * Pacific's results:
           * update our Ceph version to pacific head: [https://github.com/aquarist-labs/aquarium/issues/389](https://github.com/aquarist-labs/aquarium/issues/389)
   * Let's briefly discuss M5.
       * [https://etherpad.opensuse.org/p/aquarium-roadmap-20210506#L30](https://etherpad.opensuse.org/p/aquarium-roadmap-20210506#L30)
       * [https://github.com/aquarist-labs/aquarium/milestone/5](https://github.com/aquarist-labs/aquarium/milestone/5)
       * Pacific's results:
           * NFS HA not coming to pacific just yet, although it has been merged into master on May 25.
           * We'll focus on RGW.
               * Awesome!

* Frontend
   * Done
       * glass: Upgrade to Angular 12 - [https://github.com/aquarist-labs/aquarium/pull/507](https://github.com/aquarist-labs/aquarium/pull/507)
       * glass: Use Material CDK to copy to clipboard - [https://github.com/aquarist-labs/aquarium/pull/506](https://github.com/aquarist-labs/aquarium/pull/506)
   * In progress
       * glass: show devices step before deploying cluster - [https://github.com/aquarist-labs/aquarium/pull/509](https://github.com/aquarist-labs/aquarium/pull/509)
       * glass: Specify hostname (before bootstrap or joining) - [https://github.com/aquarist-labs/aquarium/issues/97](https://github.com/aquarist-labs/aquarium/issues/97)
       * glass: Outsource pot file change pushes - [https://github.com/aquarist-labs/aquarium/issues/264](https://github.com/aquarist-labs/aquarium/issues/264)

* Testing
   * Regular test meeting sync up?
       *  Scheduled weekly appointment with Christos, Kyr, Josh, Laura (starting next week)
   * Require successful Tumbleweed (or Leap) build for PRs to get merged (it was enabled but has been disabled some while ago for some reason)
   * Discussion on how to move forward with a coordinated testing approach should be discussed on next weeks initial testing sync meeting
       * Laura, Josh and Kyr gave us a quick overview on what is happening in regards to testing
       * we do not have a Testing plan (aka Testing Document) so far - what is everyone thinking about coming up with one?
           * positive feedback
       * Meeting could either happen as a own regular appointment or the time after the Thursday meeting can be used - TBD still
       * Testing is not just a topic for the group of three (Laura, Josh, Kyr) but also Joao and Volker should be involved - also in creating our Testing Document
       * Josh will start with some Testing Document where the rest of the group can contribute to

* Any other business
   * How to handle bind mounts on boot so there's no gap where the non-bind-mounted paths are exposed?

* AI:
